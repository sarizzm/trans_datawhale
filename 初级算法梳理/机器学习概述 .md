

# Task0-机器学习综述

2016年3月，阿尔法围棋与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜. 深度学习开始进行大众的视野中. 深度学习其实是机器学习的一个分支,我们今天来看看机器学习是什么. 机器学习是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。它是人工智能的核心，是使计算机具有智能的根本途径.  

## 机器学习的发展

![](..\图片\机器学习的发展.svg)

## 机器学习分类

![](..\图片\机器学习分类.svg)

## 机器学习模型

机器学习 = 数据（data） + 模型（model） + 优化方法（optimal strategy）

机器学习的算法导图[来源网络]

<img src="https://blog.griddynamics.com/content/images/2018/04/machinelearningalgorithms.png">

机器学习的注意事项[来源网络]
<img src=https://nanjunxiao.github.io/img/A%20few%20useful%20things%20to%20know%20about%20machine%20learning.jpg>

常见的机器学习算法

1. ![](..\图片\常见的机器学习算法.svg)


## 机器学习损失函数
1. 0-1损失函数
$$
L(y,f(x)) =
\begin{cases}
0, & \text{y = f(x)}  \\
1, & \text{y $\neq$ f(x)}
\end{cases}
$$
2. 绝对值损失函数
$$
L(y,f(x))=|y-f(x)|
$$
3. 平方损失函数
$$
L(y,f(x))=(y-f(x))^2
$$
4. log对数损失函数
$$
L(y,f(x))=log(1+e^{-yf(x)})
$$
5. 指数损失函数
$$
L(y,f(x))=exp(-yf(x))
$$
6. Hinge损失函数
$$
L(w,b)=max\{0,1-yf(x)\}
$$

## 机器学习优化方法

梯度下降是最常用的优化方法之一，它使用梯度的反方向$\nabla_\theta J(\theta)$更新参数$\theta$，使得目标函数$J(\theta)$达到最小化的一种优化方法，这种方法我们叫做梯度更新. 
1. (全量)梯度下降
$$
\theta=\theta-\eta\nabla_\theta J(\theta)
$$
2. 随机梯度下降
$$
\theta=\theta-\eta\nabla_\theta J(\theta;x^{(i)},y^{(i)})
$$
3. 小批量梯度下降
$$
\theta=\theta-\eta\nabla_\theta J(\theta;x^{(i:i+n)},y^{(i:i+n)})
$$
4. 引入动量的梯度下降
$$
\begin{cases}
v_t=\gamma v_{t-1}+\eta \nabla_\theta J(\theta)  \\
\theta=\theta-v_t
\end{cases}
$$
5. 自适应学习率的Adagrad算法
$$
\begin{cases}
g_t= \nabla_\theta J(\theta)  \\
\theta_{t+1}=\theta_{t,i}-\frac{\eta}{\sqrt{G_t+\varepsilon}} \cdot g_t
\end{cases}
$$
6. 牛顿法
$$
\theta_{t+1}=\theta_t-H^{-1}\nabla_\theta J(\theta_t)
$$
其中:
$t$: 迭代的轮数

$\eta$: 学习率

$G_t$: 前t次迭代的梯度和

$\varepsilon:$很小的数,防止除0错误

$H$: 损失函数相当于$\theta$的Hession矩阵在$\theta_t$处的估计


## 机器学习的评价指标
1. MSE(Mean Squared Error)
$$
MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}(y-f(x))^2
$$
2. MAE(Mean Absolute Error)
$$
MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}|y-f(x)|
$$
3. RMSE(Root Mean Squard Error)
$$
RMSE(y,f(x))=\frac{1}{1+MSE(y,f(x))}
$$
4. Top-k准确率
$$
Top_k(y,pre_y)=\begin{cases}
1, {y \in pre_y}  \\
0, {y \notin pre_y}
\end{cases}
$$
5. 混淆矩阵

| 混淆矩阵            | Predicted as Positive | Predicted as Negative |
| ------------------- | --------------------- | --------------------- |
| Labeled as Positive | True Positive(TP)     | False Negative(FN)    |
| Labeled as Negative | False Positive(FP)    | True Negative(TN)     |


* 真正例(True Positive, TP):真实类别为正例, 预测类别为正例
* 假负例(False Negative, FN): 真实类别为正例, 预测类别为负例
* 假正例(False Positive, FP): 真实类别为负例, 预测类别为正例 
* 真负例(True Negative, TN): 真实类别为负例, 预测类别为负例

* 真正率(True Positive Rate, TPR): 被预测为正的正样本数 / 正样本实际数
$$
TPR=\frac{TP}{TP+FN}
$$
* 假负率(False Negative Rate, FNR): 被预测为负的正样本数/正样本实际数
$$
FNR=\frac{FN}{TP+FN}
$$

* 假正率(False Positive Rate, FPR): 被预测为正的负样本数/负样本实际数，
$$
FPR=\frac{FP}{FP+TN}
$$
* 真负率(True Negative Rate, TNR): 被预测为负的负样本数/负样本实际数，
$$
TNR=\frac{TN}{FP+TN}
$$
* 准确率(Accuracy)
$$
ACC=\frac{TP+TN}{TP+FN+FP+TN}
$$
* 精准率
$$
P=\frac{TP}{TP+FP}
$$
* 召回率
$$
R=\frac{TP}{TP+FN}
$$
* F1-Score
$$
\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}
$$
* ROC

ROC曲线的横轴为“假正例率”，纵轴为“真正例率”. 以FPR为横坐标，TPR为纵坐标，那么ROC曲线就是改变各种阈值后得到的所有坐标点 (FPR,TPR) 的连线，画出来如下。红线是随机乱猜情况下的ROC，曲线越靠左上角，分类器越佳. 


* AUC(Area Under Curve)

AUC就是ROC曲线下的面积. 真实情况下，由于数据是一个一个的，阈值被离散化，呈现的曲线便是锯齿状的，当然数据越多，阈值分的越细，”曲线”越光滑. 

<img src="https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=b9cb389a68d0f703f2bf9d8e69933a58/f11f3a292df5e0feaafde78c566034a85fdf7251.jpg">

用AUC判断分类器（预测模型）优劣的标准:

- AUC = 1 是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器.
- 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值.
- AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测.

## 机器学习模型选择

1. 交叉验证

所有数据分为三部分：训练集、交叉验证集和测试集。交叉验证集不仅在选择模型时有用，在超参数选择、正则项参数 [公式] 和评价模型中也很有用。

2. k-折叠交叉验证

- 假设训练集为S ，将训练集等分为k份:$\{S_1, S_2, ..., S_k\}$. 
- 然后每次从集合中拿出k-1份进行训练
- 利用集合中剩下的那一份来进行测试并计算损失值
- 最后得到k次测试得到的损失值，并选择平均损失值最小的模型

3. Bias与Variance，欠拟合与过拟合

**欠拟合**一般表示模型对数据的表现能力不足，通常是模型的复杂度不够，并且Bias高，训练集的损失值高，测试集的损失值也高.

**过拟合**一般表示模型对数据的表现能力过好，通常是模型的复杂度过高，并且Variance高，训练集的损失值低，测试集的损失值高.

<img src="https://pic3.zhimg.com/80/v2-e20cd1183ec930a3edc94b30274be29e_hd.jpg">

<img src="https://pic1.zhimg.com/80/v2-22287dec5b6205a5cd45cf6c24773aac_hd.jpg">

4. 解决方法

- 增加训练样本: 解决高Variance情况
- 减少特征维数: 解决高Variance情况
- 增加特征维数: 解决高Bias情况
- 增加模型复杂度: 解决高Bias情况
- 减小模型复杂度: 解决高Variance情况


## 机器学习参数调优

1. 网格搜索

一种调参手段；穷举搜索：在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果

2. 随机搜索

与网格搜索相比，随机搜索并未尝试所有参数值，而是从指定的分布中采样固定数量的参数设置。它的理论依据是，如果随即样本点集足够大，那么也可以找到全局的最大或最小值，或它们的近似值。通过对搜索范围的随机取样，随机搜索一般会比网格搜索要快一些。

3. 贝叶斯优化算法

贝叶斯优化用于机器学习调参由J. Snoek(2012)提出，主要思想是，给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。


## 参考文献
1. https://www.jianshu.com/p/a95d33fea777
2. https://baike.baidu.com/item/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/217599?fr=aladdin
3. https://blog.csdn.net/gongxifacai_believe/article/details/91355237
4. https://baike.baidu.com/item/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/2971075?fr=aladdin
5. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
6. https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/
7. https://www.kaggle.com/getting-started/83518
8. https://www.cnblogs.com/lliuye/p/9549881.html
9. https://blog.csdn.net/fisherming/article/details/80209182
10. https://www.jianshu.com/p/b0f56b7d7ee8
11. https://blog.csdn.net/qq_20011607/article/details/81712811
12. https://baike.baidu.com/item/AUC/19282953?fr=aladdin
13. https://zhuanlan.zhihu.com/p/30844838
14. https://www.jianshu.com/p/55b9f2ea283b
15. https://www.jianshu.com/p/5378ef009cae
16. https://www.jianshu.com/p/5378ef009cae

**注**: 资源很多来自网络,已给出引用,如有问题请联系: xiaoranone@126.com

