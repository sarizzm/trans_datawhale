{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务3 特征工程&特征选择(3天)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#核心代码举例\n",
    "\n",
    "# # 统计特征\n",
    "#     #计算均值\n",
    "#     gp = train.groupby(by)[fea].mean()\n",
    "#     #计算中位数\n",
    "#     gp = train.groupby(by)[fea].median()\n",
    "#     #计算方差\n",
    "#     gp = train.groupby(by)[fea].std()\n",
    "#     #计算最大值\n",
    "#     gp = train.groupby(by)[fea].max()\n",
    "#     #计算最小值\n",
    "#     gp = train.groupby(by)[fea].min()\n",
    "#     #计算出现次数\n",
    "#     gp = train.groupby(by)[fea].size()\n",
    "    \n",
    "\n",
    "# # groupby生成统计特征：mean,std\n",
    "#     # 按照communityName分组计算面积的均值和方差\n",
    "#     temp = data.groupby('communityName')['area'].agg({'com_area_mean': 'mean', 'com_area_std': 'std'})\n",
    "\n",
    "# # 特征拆分\n",
    "#     # 将houseType转为'Room'，'Hall'，'Bath'\n",
    "#     def Room(x):\n",
    "#         Room = int(x.split('室')[0])\n",
    "#         return Room\n",
    "#     def Hall(x):\n",
    "#         Hall = int(x.split(\"室\")[1].split(\"厅\")[0])\n",
    "#         return Hall\n",
    "#     def Bath(x):\n",
    "#         Bath = int(x.split(\"室\")[1].split(\"厅\")[1].split(\"卫\")[0])\n",
    "#         return Bath\n",
    "\n",
    "#     data['Room'] = data['houseType'].apply(lambda x: Room(x))\n",
    "#     data['Hall'] = data['houseType'].apply(lambda x: Hall(x))\n",
    "#     data['Bath'] = data['houseType'].apply(lambda x: Bath(x))\n",
    "    \n",
    "# #特征合并\n",
    "#     # 合并部分配套设施特征\n",
    "#     data['trainsportNum'] = 5 * data['subwayStationNum'] / data['subwayStationNum'].mean() + data['busStationNum'] / \\\n",
    "#                                                                                              data[\n",
    "#                                                                                                  'busStationNum'].mean()\n",
    "\n",
    "# # 交叉生成特征:特征之间交叉+ - * / \n",
    "# data['Room_Bath'] = (data['Bath']+1) / (data['Room']+1)\n",
    "\n",
    "\n",
    "# # 聚类特征\n",
    "# from sklearn.mixture import GaussianMixture  使用GaussianMixture做聚类特征\n",
    "# gmm = GaussianMixture(n_components=4, covariance_type='full', random_state=0)\n",
    "# gmm.fit_predict(data)\n",
    " \n",
    "# # 特征编码\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# data['communityName'] = LabelEncoder().fit_transform(data['communityName'])\n",
    "# from sklearn import preprocessing.OneHotEncoder\n",
    "# data['communityName'] = OneHotEncoder().fit_transform(data['communityName'])\n",
    "\n",
    "\n",
    "# # 过大量级值取log平滑（针对线性模型有效）\n",
    "# data[feature]=np.log1p(data[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T13:40:19.692972Z",
     "start_time": "2019-12-24T13:40:19.126469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41440, 51) (2469, 50)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train = pd.read_csv('./数据集/train_data.csv')\n",
    "test = pd.read_csv('./数据集/test_a.csv')\n",
    "# print(test.columns.to_list())\n",
    "# print(train.shape,test.shape)\n",
    "\n",
    "# target_train = train['tradeMoney']\n",
    "target_test = pd.read_csv('./数据集/评分文件/sub_a_913.csv')\n",
    "# print(target_test)\n",
    "# print(target_train.head())\n",
    "print(train.shape,test.shape)\n",
    "# target_test = test.pop('tradeMoney')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理和lableEncode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingData(data):\n",
    "    # 填充缺失值\n",
    "    data['rentType'][data['rentType'] == '--'] = '未知方式'\n",
    "    \n",
    "    # 转换object类型数据\n",
    "    columns = ['rentType','communityName', 'houseFloor', 'houseToward', 'houseDecoration',  'region', 'plate'] #,'houseType'\n",
    "    \n",
    "    for feature in columns:\n",
    "        data[feature] = LabelEncoder().fit_transform(data[feature])\n",
    "\n",
    "    # 将buildYear列转换为整型数据\n",
    "    buildYearmean = pd.DataFrame(data[data['buildYear'] != '暂无信息']['buildYear'].mode())\n",
    "    data.loc[data[data['buildYear'] == '暂无信息'].index, 'buildYear'] = buildYearmean.iloc[0, 0]\n",
    "    data['buildYear'] = data['buildYear'].astype('int')\n",
    "\n",
    "    # 处理pv和uv的空值\n",
    "    data['pv'].fillna(data['pv'].mean(), inplace=True)\n",
    "    data['uv'].fillna(data['uv'].mean(), inplace=True)\n",
    "    data['pv'] = data['pv'].astype('int')\n",
    "    data['uv'] = data['uv'].astype('int')\n",
    "\n",
    "    # 分割交易时间\n",
    "    def month(x):\n",
    "        month = int(x.split('/')[1])\n",
    "        return month\n",
    "    def day(x):\n",
    "        day = int(x.split('/')[2])\n",
    "        return day\n",
    "    data['month'] = data['tradeTime'].apply(lambda x: month(x))\n",
    "#     data['day'] = data['tradeTime'].apply(lambda x: day(x))\n",
    "    \n",
    "#     # 去掉部分特征\n",
    "    data.drop('city', axis=1, inplace=True)\n",
    "    data.drop('tradeTime', axis=1, inplace=True)\n",
    "    data.drop('ID', axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "train = preprocessingData(train)\n",
    "test = preprocessingData(test)\n",
    "# print(data_train['rentType'])\n",
    "# sns.distplot([data_train['rentType']])\n",
    "# plt.hist(data_train['rentType'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-异常值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\lib\\site-packages\\sklearn\\ensemble\\iforest.py:415: DeprecationWarning: threshold_ attribute is deprecated in 0.20 and will be removed in 0.22.\n",
      "  \" be removed in 0.22.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([   62,    69,   128,   131,   246,   261,   266,   297,   308,\n",
      "              313,\n",
      "            ...\n",
      "            39224, 39228, 39319, 39347, 39352, 39434, 39563, 41080, 41083,\n",
      "            41233],\n",
      "           dtype='int64', length=405)\n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "from sklearn.ensemble import IsolationForest\n",
    "def IF_drop(train):\n",
    "    IForest = IsolationForest(contamination=0.01)\n",
    "    IForest.fit(train[\"tradeMoney\"].values.reshape(-1,1))\n",
    "    y_pred = IForest.predict(train[\"tradeMoney\"].values.reshape(-1,1))\n",
    "    drop_index = train.loc[y_pred==-1].index\n",
    "    print(drop_index)\n",
    "    train.drop(drop_index,inplace=True)\n",
    "    return train\n",
    "\n",
    "train = IF_drop(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropData(train):\n",
    "    # 丢弃部分异常值\n",
    "    train = train[train.area <= 200]\n",
    "    train = train[(train.tradeMoney <=16000) & (train.tradeMoney >=700)]\n",
    "    train.drop(train[(train['totalFloor'] == 0)].index, inplace=True)\n",
    "    return train  \n",
    "#数据集异常值处理\n",
    "train = dropData(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- 深度清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(data):\n",
    "    data.drop(data[(data['region']=='RG00001') & (data['tradeMoney']<1000)&(data['area']>50)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00001') & (data['tradeMoney']>25000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00001') & (data['area']>250)&(data['tradeMoney']<20000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00001') & (data['area']>400)&(data['tradeMoney']>50000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00001') & (data['area']>100)&(data['tradeMoney']<2000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00002') & (data['area']<100)&(data['tradeMoney']>60000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00003') & (data['area']<300)&(data['tradeMoney']>30000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00003') & (data['tradeMoney']<500)&(data['area']<50)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00003') & (data['tradeMoney']<1500)&(data['area']>100)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00003') & (data['tradeMoney']<2000)&(data['area']>300)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00003') & (data['tradeMoney']>5000)&(data['area']<20)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00003') & (data['area']>600)&(data['tradeMoney']>40000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00004') & (data['tradeMoney']<1000)&(data['area']>80)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00006') & (data['tradeMoney']<200)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00005') & (data['tradeMoney']<2000)&(data['area']>180)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00005') & (data['tradeMoney']>50000)&(data['area']<200)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00006') & (data['area']>200)&(data['tradeMoney']<2000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00007') & (data['area']>100)&(data['tradeMoney']<2500)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00010') & (data['area']>200)&(data['tradeMoney']>25000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00010') & (data['area']>400)&(data['tradeMoney']<15000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00010') & (data['tradeMoney']<3000)&(data['area']>200)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00010') & (data['tradeMoney']>7000)&(data['area']<75)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00010') & (data['tradeMoney']>12500)&(data['area']<100)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00004') & (data['area']>400)&(data['tradeMoney']>20000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00008') & (data['tradeMoney']<2000)&(data['area']>80)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00009') & (data['tradeMoney']>40000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00009') & (data['area']>300)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00009') & (data['area']>100)&(data['tradeMoney']<2000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00011') & (data['tradeMoney']<10000)&(data['area']>390)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00012') & (data['area']>120)&(data['tradeMoney']<5000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00013') & (data['area']<100)&(data['tradeMoney']>40000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00013') & (data['area']>400)&(data['tradeMoney']>50000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00013') & (data['area']>80)&(data['tradeMoney']<2000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00014') & (data['area']>300)&(data['tradeMoney']>40000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00014') & (data['tradeMoney']<1300)&(data['area']>80)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00014') & (data['tradeMoney']<8000)&(data['area']>200)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00014') & (data['tradeMoney']<1000)&(data['area']>20)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00014') & (data['tradeMoney']>25000)&(data['area']>200)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00014') & (data['tradeMoney']<20000)&(data['area']>250)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00005') & (data['tradeMoney']>30000)&(data['area']<100)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00005') & (data['tradeMoney']<50000)&(data['area']>600)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00005') & (data['tradeMoney']>50000)&(data['area']>350)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00006') & (data['tradeMoney']>4000)&(data['area']<100)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00006') & (data['tradeMoney']<600)&(data['area']>100)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00006') & (data['area']>165)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00012') & (data['tradeMoney']<800)&(data['area']<30)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00007') & (data['tradeMoney']<1100)&(data['area']>50)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00004') & (data['tradeMoney']>8000)&(data['area']<80)].index,inplace=True)\n",
    "    data.loc[(data['region']=='RG00002')&(data['area']>50)&(data['rentType']=='合租'),'rentType']='整租'\n",
    "    data.loc[(data['region']=='RG00014')&(data['rentType']=='合租')&(data['area']>60),'rentType']='整租'\n",
    "    data.drop(data[(data['region']=='RG00008')&(data['tradeMoney']>15000)&(data['area']<110)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00008')&(data['tradeMoney']>20000)&(data['area']>110)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00008')&(data['tradeMoney']<1500)&(data['area']<50)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00008')&(data['rentType']=='合租')&(data['area']>50)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00015') ].index,inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    return data\n",
    "\n",
    "train = cleanData(train)\n",
    "target_train = train.pop('tradeMoney')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T13:40:55.454321Z",
     "start_time": "2019-12-24T13:40:55.291756Z"
    }
   },
   "outputs": [],
   "source": [
    "def newfeature(data):\n",
    "\n",
    "\n",
    "    # 将houseType转为'Room'，'Hall'，'Bath'\n",
    "    def Room(x):\n",
    "        Room = int(x.split('室')[0])\n",
    "        return Room\n",
    "    def Hall(x):\n",
    "        Hall = int(x.split(\"室\")[1].split(\"厅\")[0])\n",
    "        return Hall\n",
    "    def Bath(x):\n",
    "        Bath = int(x.split(\"室\")[1].split(\"厅\")[1].split(\"卫\")[0])\n",
    "        return Bath\n",
    "\n",
    "    data['Room'] = data['houseType'].apply(lambda x: Room(x))\n",
    "    data['Hall'] = data['houseType'].apply(lambda x: Hall(x))\n",
    "    data['Bath'] = data['houseType'].apply(lambda x: Bath(x))\n",
    "    data['Room_Bath'] = (data['Bath']+1) / (data['Room']+1)\n",
    "    # 填充租房类型\n",
    "    data.loc[(data['rentType'] == '未知方式') & (data['Room'] <= 1), 'rentType'] = '整租'\n",
    "    # print(data.loc[(data['rentType']=='未知方式')&(data['Room_Bath']>1),'rentType'])\n",
    "    data.loc[(data['rentType'] == '未知方式') & (data['Room_Bath'] > 1), 'rentType'] = '合租'\n",
    "    data.loc[(data['rentType'] == '未知方式') & (data['Room'] > 1) & (data['area'] < 50), 'rentType'] = '合租'\n",
    "    data.loc[(data['rentType'] == '未知方式') & (data['area'] / data['Room'] < 20), 'rentType'] = '合租'\n",
    "    # data.loc[(data['rentType']=='未知方式')&(data['area']>60),'rentType']='合租'\n",
    "    data.loc[(data['rentType'] == '未知方式') & (data['area'] <= 50) & (data['Room'] == 2), 'rentType'] = '合租'\n",
    "    data.loc[(data['rentType'] == '未知方式') & (data['area'] > 60) & (data['Room'] == 2), 'rentType'] = '整租'\n",
    "    data.loc[(data['rentType'] == '未知方式') & (data['area'] <= 60) & (data['Room'] == 3), 'rentType'] = '合租'\n",
    "    data.loc[(data['rentType'] == '未知方式') & (data['area'] > 60) & (data['Room'] == 3), 'rentType'] = '整租'\n",
    "    data.loc[(data['rentType'] == '未知方式') & (data['area'] >= 100) & (data['Room'] > 3), 'rentType'] = '整租'\n",
    "\n",
    "    # data.drop('Room_Bath', axis=1, inplace=True)\n",
    "    # 提升0.0001\n",
    "#     def month(x):\n",
    "#         month = int(x.split('/')[1])\n",
    "#         return month\n",
    "#     def day(x):\n",
    "#         day = int(x.split('/')[2])\n",
    "#         return day\n",
    "    # 结果变差\n",
    "\n",
    "    # 分割交易时间\n",
    "#     data['year']=data['tradeTime'].apply(lambda x:year(x))\n",
    "#     data['month'] = data['tradeTime'].apply(lambda x: month(x))\n",
    "#     data['day'] = data['tradeTime'].apply(lambda x: day(x))# 结果变差\n",
    "    #     data['pv/uv'] = data['pv'] / data['uv']\n",
    "    #     data['房间总数'] = data['室'] + data['厅'] + data['卫']\n",
    "\n",
    "    # 合并部分配套设施特征\n",
    "    data['trainsportNum'] = 5 * data['subwayStationNum'] / data['subwayStationNum'].mean() + data['busStationNum'] / \\\n",
    "                                                                                             data[\n",
    "                                                                                                 'busStationNum'].mean()\n",
    "    data['all_SchoolNum'] = 2 * data['interSchoolNum'] / data['interSchoolNum'].mean() + data['schoolNum'] / data[\n",
    "        'schoolNum'].mean() \\\n",
    "                            + data['privateSchoolNum'] / data['privateSchoolNum'].mean()\n",
    "    data['all_hospitalNum'] = 2 * data['hospitalNum'] / data['hospitalNum'].mean() + \\\n",
    "                              data['drugStoreNum'] / data['drugStoreNum'].mean()\n",
    "    data['all_mall'] = data['mallNum'] / data['mallNum'].mean() + \\\n",
    "                       data['superMarketNum'] / data['superMarketNum'].mean()\n",
    "    data['otherNum'] = data['gymNum'] / data['gymNum'].mean() + data['bankNum'] / data['bankNum'].mean() + \\\n",
    "                       data['shopNum'] / data['shopNum'].mean() + 2 * data['parkNum'] / data['parkNum'].mean()\n",
    "\n",
    "    data.drop(['subwayStationNum', 'busStationNum',\n",
    "               'interSchoolNum', 'schoolNum', 'privateSchoolNum',\n",
    "               'hospitalNum', 'drugStoreNum', 'mallNum', 'superMarketNum', 'gymNum', 'bankNum', 'shopNum', 'parkNum'],\n",
    "              axis=1, inplace=True)\n",
    "    # 提升0.0005\n",
    "    \n",
    "#     data['houseType_1sumcsu']=data['Bath'].map(lambda x:str(x))+data['month'].map(lambda x:str(x))\n",
    "#     data['houseType_2sumcsu']=data['Bath'].map(lambda x:str(x))+data['communityName']\n",
    "#     data['houseType_3sumcsu']=data['Bath'].map(lambda x:str(x))+data['plate']\n",
    "    \n",
    "    data.drop('houseType', axis=1, inplace=True)\n",
    "#     data.drop('tradeTime', axis=1, inplace=True)\n",
    "    \n",
    "    data[\"area\"] = data[\"area\"].astype(int)\n",
    "\n",
    "\n",
    "    # categorical_feats = ['rentType', 'houseFloor', 'houseToward', 'houseDecoration', 'communityName','region', 'plate']\n",
    "    categorical_feats = ['rentType', 'houseFloor', 'houseToward', 'houseDecoration',  'region', 'plate','cluster']\n",
    "\n",
    "    return data, categorical_feats\n",
    "\n",
    "train,_ = newfeature(train)\n",
    "test,_ = newfeature(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T13:41:02.458588Z",
     "start_time": "2019-12-24T13:41:00.981539Z"
    }
   },
   "outputs": [],
   "source": [
    "#计算统计特征\n",
    "def featureCount(train,test):\n",
    "    train['data_type'] = 0\n",
    "    test['data_type'] = 1\n",
    "    data = pd.concat([train, test], axis=0, join='outer')\n",
    "    def feature_count(data, features=[]):\n",
    "        new_feature = 'count'\n",
    "        for i in features:\n",
    "            new_feature += '_' + i\n",
    "        temp = data.groupby(features).size().reset_index().rename(columns={0: new_feature})\n",
    "        data = data.merge(temp, 'left', on=features)\n",
    "        return data\n",
    "\n",
    "    data = feature_count(data, ['communityName'])\n",
    "    data = feature_count(data, ['buildYear'])\n",
    "    data = feature_count(data, ['totalFloor'])\n",
    "    data = feature_count(data, ['communityName', 'totalFloor'])\n",
    "    data = feature_count(data, ['communityName', 'newWorkers'])\n",
    "    data = feature_count(data, ['communityName', 'totalTradeMoney'])\n",
    "    new_train = data[data['data_type'] == 0]\n",
    "    new_test = data[data['data_type'] == 1]\n",
    "    new_train.drop('data_type', axis=1, inplace=True)\n",
    "    new_test.drop(['data_type'], axis=1, inplace=True)\n",
    "    return new_train, new_test\n",
    "    \n",
    "train, test = featureCount(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupby方法生成统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T13:41:05.546332Z",
     "start_time": "2019-12-24T13:41:04.242821Z"
    }
   },
   "outputs": [],
   "source": [
    "#groupby生成统计特征：mean,std等\n",
    "\n",
    "def gourpby(train,test):\n",
    "    train['data_type'] = 0\n",
    "    test['data_type'] = 1\n",
    "    data = pd.concat([train, test], axis=0, join='outer')\n",
    "    columns = ['rentType', 'houseFloor', 'houseToward', 'houseDecoration', 'communityName', 'region', 'plate']\n",
    "    for feature in columns:\n",
    "        data[feature] = LabelEncoder().fit_transform(data[feature])\n",
    "\n",
    "    temp = data.groupby('communityName')['area'].agg({'com_area_mean': 'mean', 'com_area_std': 'std'})\n",
    "    temp.fillna(0, inplace=True)\n",
    "    data = data.merge(temp, on='communityName', how='left')\n",
    "    \n",
    "    data['price_per_area'] = data.tradeMeanPrice / data.area * 100\n",
    "    temp = data.groupby('communityName')['price_per_area'].agg(\n",
    "        {'comm_price_mean': 'mean', 'comm_price_std': 'std'})\n",
    "    temp.fillna(0, inplace=True)\n",
    "    data = data.merge(temp, on='communityName', how='left')\n",
    "   \n",
    "    temp = data.groupby('plate')['price_per_area'].agg(\n",
    "        {'plate_price_mean': 'mean', 'plate_price_std': 'std'})\n",
    "    temp.fillna(0, inplace=True)\n",
    "    data = data.merge(temp, on='plate', how='left')\n",
    "    data.drop('price_per_area', axis=1, inplace=True)\n",
    "\n",
    "    temp = data.groupby('plate')['area'].agg({'plate_area_mean': 'mean', 'plate_area_std': 'std'})\n",
    "    temp.fillna(0, inplace=True)\n",
    "    data = data.merge(temp, on='plate', how='left')\n",
    "    \n",
    "    temp = data.groupby(['plate'])['buildYear'].agg({'plate_year_mean': 'mean', 'plate_year_std': 'std'})\n",
    "    data = data.merge(temp, on='plate', how='left')\n",
    "    data.plate_year_mean = data.plate_year_mean.astype('int')\n",
    "    data['comm_plate_year_diff'] = data.buildYear - data.plate_year_mean\n",
    "    data.drop('plate_year_mean', axis=1, inplace=True)\n",
    "\n",
    "    temp = data.groupby('plate')['trainsportNum'].agg('sum').reset_index(name='plate_trainsportNum')\n",
    "    data = data.merge(temp, on='plate', how='left')\n",
    "    temp = data.groupby(['communityName', 'plate'])['trainsportNum'].agg('sum').reset_index(name='com_trainsportNum')\n",
    "    data = data.merge(temp, on=['communityName', 'plate'], how='left')\n",
    "    data['trainsportNum_ratio'] = list(map(lambda x, y: round(x / y, 3) if y != 0 else -1,\n",
    "                                           data['com_trainsportNum'], data['plate_trainsportNum']))\n",
    "    data = data.drop(['com_trainsportNum', 'plate_trainsportNum'], axis=1)\n",
    "\n",
    "    temp = data.groupby('plate')['all_SchoolNum'].agg('sum').reset_index(name='plate_all_SchoolNum')\n",
    "    data = data.merge(temp, on='plate', how='left')\n",
    "    temp = data.groupby(['communityName', 'plate'])['all_SchoolNum'].agg('sum').reset_index(name='com_all_SchoolNum')\n",
    "    data = data.merge(temp, on=['communityName', 'plate'], how='left')\n",
    "    data = data.drop(['com_all_SchoolNum', 'plate_all_SchoolNum'], axis=1)\n",
    "\n",
    "    temp = data.groupby(['communityName', 'plate'])['all_mall'].agg('sum').reset_index(name='com_all_mall')\n",
    "    data = data.merge(temp, on=['communityName', 'plate'], how='left')\n",
    "\n",
    "    temp = data.groupby('plate')['otherNum'].agg('sum').reset_index(name='plate_otherNum')\n",
    "    data = data.merge(temp, on='plate', how='left')\n",
    "    temp = data.groupby(['communityName', 'plate'])['otherNum'].agg('sum').reset_index(name='com_otherNum')\n",
    "    data = data.merge(temp, on=['communityName', 'plate'], how='left')\n",
    "    data['other_ratio'] = list(map(lambda x, y: round(x / y, 3) if y != 0 else -1,\n",
    "                                   data['com_otherNum'], data['plate_otherNum']))\n",
    "    data = data.drop(['com_otherNum', 'plate_otherNum'], axis=1)\n",
    "\n",
    "    temp = data.groupby(['month', 'communityName']).size().reset_index(name='communityName_saleNum')\n",
    "    data = data.merge(temp, on=['month', 'communityName'], how='left')\n",
    "    temp = data.groupby(['month', 'plate']).size().reset_index(name='plate_saleNum')\n",
    "    data = data.merge(temp, on=['month', 'plate'], how='left')\n",
    "\n",
    "    data['sale_ratio'] = round((data.communityName_saleNum + 1) / (data.plate_saleNum + 1), 3)\n",
    "    data['sale_newworker_differ'] = 3 * data.plate_saleNum - data.newWorkers\n",
    "    data.drop(['communityName_saleNum', 'plate_saleNum'], axis=1, inplace=True)\n",
    "\n",
    "    new_train = data[data['data_type'] == 0]\n",
    "    new_test = data[data['data_type'] == 1]\n",
    "    new_train.drop('data_type', axis=1, inplace=True)\n",
    "    new_test.drop(['data_type'], axis=1, inplace=True)\n",
    "    return new_train, new_test\n",
    "\n",
    "train, test = gourpby(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T13:38:33.198959Z",
     "start_time": "2019-12-24T13:38:33.193970Z"
    }
   },
   "source": [
    "## 聚类方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T13:41:25.894916Z",
     "start_time": "2019-12-24T13:41:25.241666Z"
    }
   },
   "outputs": [],
   "source": [
    "#聚类\n",
    "def cluster(train,test):\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "\n",
    "    train['data_type'] = 0\n",
    "    test['data_type'] = 1\n",
    "    data = pd.concat([train, test], axis=0, join='outer')\n",
    "    col = ['totalFloor',\n",
    "           'houseDecoration', 'communityName', 'region', 'plate', 'buildYear',\n",
    "\n",
    "           'tradeMeanPrice', 'tradeSecNum', 'totalNewTradeMoney',\n",
    "           'totalNewTradeArea', 'tradeNewMeanPrice', 'tradeNewNum', 'remainNewNum',\n",
    "\n",
    "           'landTotalPrice', 'landMeanPrice', 'totalWorkers',\n",
    "           'newWorkers', 'residentPopulation', 'lookNum',\n",
    "           'trainsportNum',\n",
    "           'all_SchoolNum', 'all_hospitalNum', 'all_mall', 'otherNum']\n",
    "\n",
    "    # EM\n",
    "    gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=0)\n",
    "    data['cluster']= pd.DataFrame(gmm.fit_predict(data[col]))\n",
    "\n",
    "\n",
    "    col1 = ['totalFloor','houseDecoration', 'communityName', 'region', 'plate', 'buildYear']\n",
    "    col2 = ['tradeMeanPrice', 'tradeSecNum', 'totalNewTradeMoney',\n",
    "            'totalNewTradeArea', 'tradeNewMeanPrice', 'tradeNewNum', 'remainNewNum',\n",
    "            'landTotalPrice', 'landMeanPrice', 'totalWorkers',\n",
    "            'newWorkers', 'residentPopulation', 'lookNum',\n",
    "            'trainsportNum',\n",
    "            'all_SchoolNum', 'all_hospitalNum', 'all_mall', 'otherNum']\n",
    "    for feature1 in col1:\n",
    "        for feature2 in col2:\n",
    "        \n",
    "            temp = data.groupby(['cluster',feature1])[feature2].agg('mean').reset_index(name=feature2+'_'+feature1+'_cluster_mean')\n",
    "            temp.fillna(0, inplace=True)\n",
    "       \n",
    "            data = data.merge(temp, on=['cluster', feature1], how='left')\n",
    "    \n",
    "   \n",
    "    new_train = data[data['data_type'] == 0]\n",
    "    new_test = data[data['data_type'] == 1]\n",
    "    new_train.drop('data_type', axis=1, inplace=True)\n",
    "    new_test.drop(['data_type'], axis=1, inplace=True)\n",
    "    \n",
    "    return new_train, new_test\n",
    "\n",
    "train, test = cluster(train, test)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log平滑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2000.0\n",
      "1     2000.0\n",
      "2    16000.0\n",
      "3     1600.0\n",
      "4     2900.0\n",
      "Name: tradeMoney, dtype: float64\n",
      "(40215, 173) (2469, 173)\n"
     ]
    }
   ],
   "source": [
    "# 过大量级值取log平滑（针对线性模型有效）\n",
    "big_num_cols = ['totalTradeMoney','totalTradeArea','tradeMeanPrice','totalNewTradeMoney', 'totalNewTradeArea',\n",
    "                'tradeNewMeanPrice','remainNewNum', 'supplyNewNum', 'supplyLandArea',\n",
    "                'tradeLandArea','landTotalPrice','landMeanPrice','totalWorkers','newWorkers',\n",
    "                'residentPopulation','pv','uv']\n",
    "for col in big_num_cols:\n",
    "        train[col] = train[col].map(lambda x: np.log1p(x))\n",
    "        test[col] = test[col].map(lambda x: np.log1p(x))\n",
    "\n",
    "        \n",
    "print(target_train.head())\n",
    "print(train.shape,test.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40215,)\n",
      "   area  rentType  houseFloor  totalFloor  houseToward  houseDecoration  \\\n",
      "0    68         2           1          16            6                0   \n",
      "1   125         2           0          14            6                2   \n",
      "2   132         2           1          32            6                0   \n",
      "3    57         2           0          17            6                3   \n",
      "4   129         2           1           2            6                1   \n",
      "\n",
      "   communityName  region  plate  buildYear  ...  \\\n",
      "0             50       0     63       1953  ...   \n",
      "1            129       1     48       2007  ...   \n",
      "2            178       1     49       1994  ...   \n",
      "3            312       1     50       1994  ...   \n",
      "4           1256       2     43       1994  ...   \n",
      "\n",
      "   landMeanPrice_buildYear_cluster_mean  totalWorkers_buildYear_cluster_mean  \\\n",
      "0                              0.000000                        193780.461538   \n",
      "1                            795.452036                         48519.514673   \n",
      "2                            466.059740                         60265.621118   \n",
      "3                            466.059740                         60265.621118   \n",
      "4                            466.059740                         60265.621118   \n",
      "\n",
      "   newWorkers_buildYear_cluster_mean  \\\n",
      "0                        7624.692308   \n",
      "1                        4622.501129   \n",
      "2                        2451.548447   \n",
      "3                        2451.548447   \n",
      "4                        2451.548447   \n",
      "\n",
      "   residentPopulation_buildYear_cluster_mean  lookNum_buildYear_cluster_mean  \\\n",
      "0                              241618.461538                        3.538462   \n",
      "1                              300307.914221                        1.948081   \n",
      "2                              345217.948447                        1.295031   \n",
      "3                              345217.948447                        1.295031   \n",
      "4                              345217.948447                        1.295031   \n",
      "\n",
      "   trainsportNum_buildYear_cluster_mean  all_SchoolNum_buildYear_cluster_mean  \\\n",
      "0                              6.239765                              3.650874   \n",
      "1                              5.271092                              3.518400   \n",
      "2                              7.833503                              3.674482   \n",
      "3                              7.833503                              3.674482   \n",
      "4                              7.833503                              3.674482   \n",
      "\n",
      "   all_hospitalNum_buildYear_cluster_mean  all_mall_buildYear_cluster_mean  \\\n",
      "0                                2.966036                         1.499059   \n",
      "1                                3.131424                         1.822096   \n",
      "2                                3.772344                         2.413125   \n",
      "3                                3.772344                         2.413125   \n",
      "4                                3.772344                         2.413125   \n",
      "\n",
      "   otherNum_buildYear_cluster_mean  \n",
      "0                         4.199451  \n",
      "1                         4.729657  \n",
      "2                         6.340695  \n",
      "3                         6.340695  \n",
      "4                         6.340695  \n",
      "\n",
      "[5 rows x 173 columns]\n",
      "       area  rentType  houseFloor  totalFloor  houseToward  houseDecoration  \\\n",
      "40215    36         2           2          21            6                0   \n",
      "40216    64         2           2          11            3                0   \n",
      "40217    98         2           2           6            4                3   \n",
      "40218    43         2           2           6            4                0   \n",
      "40219    92         2           2           6            4                0   \n",
      "\n",
      "       communityName  region  plate  buildYear  ...  \\\n",
      "40215           2236      11     11       2008  ...   \n",
      "40216            494       1     52       2009  ...   \n",
      "40217            512       1     52       1996  ...   \n",
      "40218            476       1     52       1994  ...   \n",
      "40219            770       1     58       2004  ...   \n",
      "\n",
      "       landMeanPrice_buildYear_cluster_mean  \\\n",
      "40215                            655.458055   \n",
      "40216                           1151.993716   \n",
      "40217                            120.940379   \n",
      "40218                            310.291571   \n",
      "40219                            298.289438   \n",
      "\n",
      "       totalWorkers_buildYear_cluster_mean  newWorkers_buildYear_cluster_mean  \\\n",
      "40215                         52089.265909                            4280.45   \n",
      "40216                         60453.126667                            4366.84   \n",
      "40217                         82070.736979                               0.00   \n",
      "40218                         55918.365913                               0.00   \n",
      "40219                         62071.708220                               0.00   \n",
      "\n",
      "       residentPopulation_buildYear_cluster_mean  \\\n",
      "40215                              331956.852273   \n",
      "40216                              418350.346667   \n",
      "40217                              268211.722656   \n",
      "40218                              282527.065646   \n",
      "40219                              338312.512195   \n",
      "\n",
      "       lookNum_buildYear_cluster_mean  trainsportNum_buildYear_cluster_mean  \\\n",
      "40215                        1.684091                              6.715027   \n",
      "40216                        1.460000                              7.974914   \n",
      "40217                        0.000000                              6.186862   \n",
      "40218                        0.000000                              5.546013   \n",
      "40219                        0.000000                              6.622113   \n",
      "\n",
      "       all_SchoolNum_buildYear_cluster_mean  \\\n",
      "40215                              3.135605   \n",
      "40216                              4.100480   \n",
      "40217                              4.708650   \n",
      "40218                              4.072240   \n",
      "40219                              4.511442   \n",
      "\n",
      "       all_hospitalNum_buildYear_cluster_mean  \\\n",
      "40215                                3.477674   \n",
      "40216                                4.095607   \n",
      "40217                                2.315854   \n",
      "40218                                2.682411   \n",
      "40219                                3.362786   \n",
      "\n",
      "       all_mall_buildYear_cluster_mean  otherNum_buildYear_cluster_mean  \n",
      "40215                         2.152905                         4.926794  \n",
      "40216                         3.022489                         7.730457  \n",
      "40217                         1.720467                         4.401359  \n",
      "40218                         1.940263                         4.484815  \n",
      "40219                         2.171620                         5.347893  \n",
      "\n",
      "[5 rows x 173 columns]\n"
     ]
    }
   ],
   "source": [
    "print(target_train.shape)\n",
    "# print(target_train)\n",
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集结果： 0.7270259992174979\n",
      "测试集结果： 0.306681676974662\n"
     ]
    }
   ],
   "source": [
    "#对比特征工程前后线性模型结果情况\n",
    "test=test.fillna(0)\n",
    "# Lasso回归\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso=Lasso(alpha=0.1)\n",
    "lasso.fit(train,target_train)\n",
    "#预测测试集和训练集结果\n",
    "y_pred_train=lasso.predict(train)\n",
    "y_pred_test=lasso.predict(test)\n",
    "\n",
    "#对比结果\n",
    "from sklearn.metrics import r2_score\n",
    "score_train=r2_score(y_pred_train,target_train)\n",
    "print(\"训练集结果：\",score_train)\n",
    "score_test=r2_score(y_pred_test, target_test)\n",
    "print(\"测试集结果：\",score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:31:08.989972Z",
     "start_time": "2019-12-24T12:31:08.986978Z"
    }
   },
   "source": [
    "# 模型选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   机器学习模型的调参。有两种常用的调参方法：网格搜索和随机搜索。每一种都有自己的优点和缺点。网格搜索速度慢，但在搜索整个搜索空间方面效果很好，而随机搜索很快，但可能会错过搜索空间中的重要点。幸运的是，还有第三种选择：贝叶斯优化。本文我们将重点介绍贝叶斯优化的一个实现，一个名为hyperopt的 Python 模块。\n",
    "    本文是对Parameter Tuning with Hyperopt一文的翻译。译者在设计深度学习模型的网络结构发现了hyperopt这个大杀器，相比每次手动各种试，用工具批量调节网络中的各种超参数确实能省心不少。不过hyperopt的官方文档描述的太渣，google 了一翻，发现这篇博客算是介绍的比较清楚的一个，便顺手翻译了，译文已取得原作者授权。\n",
    "\n",
    "hyperopt详细中文解释：https://www.jianshu.com/p/35eed1567463"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   模型选择的标准是尽可能地贴近样本真实的分布。但是在有限的样本下，如果我们有多个可选模型，比如从简单到复杂，从低阶到高阶，参数由少到多。那么我们怎么选择模型呢，是对训练样本的拟合度越好就可以吗？显然不是，因为这样做的话只会让我们最终选择出最复杂，最高阶的模型。而这个模型的问题是过拟合的，即对样本真实分布的预测误差是很高的。那么该如何选择模型，使得泛化误差尽量小呢，有下面这些常用的方法：\n",
    "### 保留交叉验证        \n",
    "把样本分成训练样本和测试样本，一般可以7比3的比例。7成的样本训练出的模型，用3成的样本做检验。取测试准确率最高的模型\n",
    "###  K折交叉验证\n",
    "取K为10为例，把所有样本平均分成10分，然后用9份训练，剩下的1份做测试。这样可以做十次测试，取十次测试的准确率的平均值最高的模型做为选取的模型。\n",
    "###  留1交叉验证\n",
    "当样本数目很少并且很难取得的时候，K折交叉验证的极限就是让K等于样本数目N，这样N-1个样本作为训练样本，1个作为测试样本。经过N次测试，取平局准确率最高的模型作为我们选择的模型。\n",
    "在交叉验证做完选取理想模型之后，可以把所有样本再放到模型中训练一次，作为最后的输出模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train\n",
    "Y_train = target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \n",
      "LightGBM objective call #1 cur_best_score=0.00000\n",
      "Params: bagging_fraction=0.8401944845661764 bagging_freq=9.0 feature_fraction=0.9004764865751869 lambda_l1=2.2795304243957815 lambda_l2=0.6854493989713117 learning_rate=0.005817151174194069 max_bin=192.0 min_data_in_leaf=40.0 min_sum_hessian_in_leaf=2.0422427668137093 num_leaves=44.0\n",
      "nb_trees=5512 val_loss=OrderedDict([('rmse', 792.9034534207018)])                                                      \n",
      "nb_trees=6443 val_loss=OrderedDict([('rmse', 803.0143027271312)])                                                      \n",
      "nb_trees=4770 val_loss=OrderedDict([('rmse', 763.1559140197394)])                                                      \n",
      "val_r2_score=0.8898141919430032                                                                                        \n",
      "NEW BEST SCORE=0.8898141919430032                                                                                      \n",
      "                                                                                                                       \n",
      "LightGBM objective call #2 cur_best_score=0.88981\n",
      "Params: bagging_fraction=0.7529096837682713 bagging_freq=4.0 feature_fraction=0.8951686282546137 lambda_l1=3.830690375698771 lambda_l2=4.589537256557779 learning_rate=0.00621481996929681 max_bin=187.0 min_data_in_leaf=42.0 min_sum_hessian_in_leaf=1.069975008115167 num_leaves=40.0\n",
      "nb_trees=5624 val_loss=OrderedDict([('rmse', 791.8441674959295)])                                                      \n",
      "nb_trees=5305 val_loss=OrderedDict([('rmse', 805.9733130573578)])                                                      \n",
      "nb_trees=5319 val_loss=OrderedDict([('rmse', 763.8228177372165)])                                                      \n",
      "val_r2_score=0.8895074663698467                                                                                        \n",
      "                                                                                                                       \n",
      "LightGBM objective call #3 cur_best_score=0.88981\n",
      "Params: bagging_fraction=0.8361092014925472 bagging_freq=12.0 feature_fraction=0.9101505537010632 lambda_l1=3.6005339197734196 lambda_l2=8.193881787179745 learning_rate=0.00327907040286222 max_bin=91.0 min_data_in_leaf=78.0 min_sum_hessian_in_leaf=1.8098698389333743 num_leaves=85.0\n",
      "nb_trees=6300 val_loss=OrderedDict([('rmse', 789.4855904605246)])                                                      \n",
      "nb_trees=7027 val_loss=OrderedDict([('rmse', 810.9225987331946)])                                                      \n",
      "  0%|▏                                          | 2/500 [11:03<30:30:38, 220.56s/trial, best loss: -0.8898141919430032]"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "import numpy\n",
    "import hyperopt\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "import colorama\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N_HYPEROPT_PROBES = 500\n",
    "HYPEROPT_ALGO = tpe.suggest  #  tpe.suggest OR hyperopt.rand.suggest\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "colorama.init()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def get_lgb_params(space):\n",
    "    lgb_params = dict()\n",
    "    lgb_params['boosting_type'] = space['boosting_type'] if 'boosting_type' in space else 'gbdt'\n",
    "    lgb_params['objective'] = 'regression'\n",
    "    lgb_params['metric'] = 'rmse'\n",
    "    lgb_params['learning_rate'] = space['learning_rate']\n",
    "    lgb_params['num_leaves'] = int(space['num_leaves'])\n",
    "    lgb_params['min_data_in_leaf'] = int(space['min_data_in_leaf'])\n",
    "    lgb_params['min_sum_hessian_in_leaf'] = space['min_sum_hessian_in_leaf']\n",
    "    lgb_params['max_depth'] = -1\n",
    "    lgb_params['lambda_l1'] = space['lambda_l1'] if 'lambda_l1' in space else 0.0\n",
    "    lgb_params['lambda_l2'] = space['lambda_l2'] if 'lambda_l2' in space else 0.0\n",
    "    lgb_params['max_bin'] = int(space['max_bin']) if 'max_bin' in space else 256\n",
    "    lgb_params['feature_fraction'] = space['feature_fraction']\n",
    "    lgb_params['bagging_fraction'] = space['bagging_fraction']\n",
    "    lgb_params['bagging_freq'] = int(space['bagging_freq']) if 'bagging_freq' in space else 1\n",
    "    lgb_params['nthread'] = 4\n",
    "    return lgb_params\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "obj_call_count = 0\n",
    "cur_best_score = 0 # 0 or np.inf\n",
    "# log_writer = open( '../log/lgb-hyperopt-log.txt', 'w' )\n",
    "\n",
    "\n",
    "def objective(space):\n",
    "    global obj_call_count, cur_best_score\n",
    "\n",
    "    obj_call_count += 1\n",
    "\n",
    "    print('\\nLightGBM objective call #{} cur_best_score={:7.5f}'.format(obj_call_count,cur_best_score) )\n",
    "\n",
    "    lgb_params = get_lgb_params(space)\n",
    "\n",
    "    sorted_params = sorted(space.items(), key=lambda z: z[0])\n",
    "    params_str = str.join(' ', ['{}={}'.format(k, v) for k, v in sorted_params])\n",
    "    print('Params: {}'.format(params_str) )\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "    out_of_fold = np.zeros(len(X_train))\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        D_train = lgb.Dataset(X_train.iloc[train_idx], label=Y_train[train_idx])\n",
    "        D_val = lgb.Dataset(X_train.iloc[val_idx], label=Y_train[val_idx])\n",
    "        # Train\n",
    "        num_round = 10000\n",
    "        clf = lgb.train(lgb_params,\n",
    "                           D_train,\n",
    "                           num_boost_round=num_round,\n",
    "                           # metrics='mlogloss',\n",
    "                           valid_sets=D_val,\n",
    "                           # valid_names='val',\n",
    "                           # fobj=None,\n",
    "                           # feval=None,\n",
    "                           # init_model=None,\n",
    "                           # feature_name='auto',\n",
    "                           # categorical_feature='auto',\n",
    "                           early_stopping_rounds=200,\n",
    "                           # evals_result=None,\n",
    "                           verbose_eval=False,\n",
    "                           # learning_rates=None,\n",
    "                           # keep_training_booster=False,\n",
    "                           # callbacks=None\n",
    "                           )\n",
    "        # predict\n",
    "        nb_trees = clf.best_iteration\n",
    "        val_loss = clf.best_score['valid_0']\n",
    "        print('nb_trees={} val_loss={}'.format(nb_trees, val_loss))\n",
    "        out_of_fold[val_idx] = clf.predict(X_train.iloc[val_idx], num_iteration=nb_trees)\n",
    "        score = r2_score(out_of_fold, Y_train)\n",
    "\n",
    "    print('val_r2_score={}'.format(score))\n",
    "\n",
    "#     log_writer.write('score={} Params:{} nb_trees={}\\n'.format(score, params_str, nb_trees ))\n",
    "#     log_writer.flush()\n",
    "\n",
    "    if score>cur_best_score:\n",
    "        cur_best_score = score\n",
    "        print(colorama.Fore.GREEN + 'NEW BEST SCORE={}'.format(cur_best_score) + colorama.Fore.RESET)\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "space ={\n",
    "        'num_leaves': hp.quniform ('num_leaves', 10, 100, 1),\n",
    "        'min_data_in_leaf':  hp.quniform ('min_data_in_leaf', 10, 100, 1),\n",
    "        'feature_fraction': hp.uniform('feature_fraction', 0.75, 1.0),\n",
    "        'bagging_fraction': hp.uniform('bagging_fraction', 0.75, 1.0),\n",
    "        'learning_rate': hp.uniform('learning_rate', 0, 0.01),\n",
    "#         'learning_rate': hp.loguniform('learning_rate', -5.0, -2.3),\n",
    "        'min_sum_hessian_in_leaf': hp.loguniform('min_sum_hessian_in_leaf', 0, 2.3),\n",
    "        'max_bin': hp.quniform ('max_bin', 88, 200, 1),\n",
    "        'bagging_freq': hp.quniform ('bagging_freq', 1, 15, 1),\n",
    "        'lambda_l1': hp.uniform('lambda_l1', 0, 10 ),\n",
    "        'lambda_l2': hp.uniform('lambda_l2', 0, 10 ),\n",
    "       }\n",
    "\n",
    "trials = Trials()\n",
    "best = hyperopt.fmin(fn=objective,\n",
    "                     space=space,\n",
    "                     algo=HYPEROPT_ALGO,\n",
    "                     max_evals=N_HYPEROPT_PROBES,\n",
    "                     trials=trials,\n",
    "                     verbose=1)\n",
    "\n",
    "print('-'*50)\n",
    "print('The best params:')\n",
    "print( best )\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
